{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2bf3bb2",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39011040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73cdb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc38b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a980702",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8fa6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3408954",
   "metadata": {},
   "source": [
    "#### Remove white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b616b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    cl_text = text.astype(str).str.strip()\n",
    "    return cl_text.apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cfa6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate comment + rule so features can “see” both sides\n",
    "train_text = clean(train[\"body\"]) + \" [RULE] \" + clean(train[\"rule\"])\n",
    "test_text  = clean(test[\"body\"])  + \" [RULE] \" + clean(test[\"rule\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb085f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[\"rule_violation\"].astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd431579",
   "metadata": {},
   "source": [
    "#### Defining TF-IDF vectorization hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d536ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_NGRAMS = (1,2) # Use unigrams (single words) and bigrams\n",
    "MIN_DF = 2 # Ignore terms that occur in fewer than 2 documents\n",
    "MAX_DF = 0.97 # Ignore terms that appear in more than 97% of documents.\n",
    "MAXF_WORD = 150_000 # Cap the vocabulary size for word-based TF-IDF features to 150k most\n",
    "K_SELECT = 200_000 # After combining word, keep only the top 200k most discriminative features using χ² (Chi-squared) selection.\n",
    "C_GRID = [1.0, 3.0, 6.0]  # Regularization strengths for Logistic Regression\n",
    "N_SPLITS = 5 # Number of folds for cross-validation.\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d5820",
   "metadata": {},
   "source": [
    "#### Defining XGBOOST parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6fb9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost params kept simple & robust for sparse TF‑IDF\n",
    "XGB_PARAMS = dict(\n",
    "    n_estimators=400,            # enough trees; early stopping will cut this if needed\n",
    "    learning_rate=0.08,          # conservative LR\n",
    "    max_depth=6,                 # moderate depth\n",
    "    subsample=0.8,               # regularization\n",
    "    colsample_bytree=0.8,        # regularization\n",
    "    reg_lambda=1.0,              # L2\n",
    "    eval_metric=\"auc\",           # AUC across folds\n",
    "    tree_method=\"hist\",          # fast CPU histogram algorithm\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa7e2c",
   "metadata": {},
   "source": [
    "#### Defining 2 feature variants to compare under CV protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f69bd7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = {\n",
    "    \"A_word_only_noSW\": {\"stopwords\": None},\n",
    "    \"B_word_only_SW\"  : {\"stopwords\": \"english\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6407d0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A_word_only_noSW] XGB mean AUC=0.7825 (±0.0179); best iters per fold: [400, 400, 400, 400, 400]\n",
      "[B_word_only_SW] XGB mean AUC=0.7937 (±0.0185); best iters per fold: [400, 400, 400, 400, 400]\n",
      "\n",
      ">>> WINNER: B_word_only_SW | XGB CV mean AUC = 0.7937 (±0.0185)\n",
      "Saved: ../outputs/sub_v2_tfidf_xgb.csv\n",
      "   row_id  rule_violation\n",
      "0    2029        0.142839\n",
      "1    2030        0.195030\n",
      "2    2031        0.839390\n",
      "3    2032        0.480992\n",
      "4    2033        0.787956\n"
     ]
    }
   ],
   "source": [
    "def build_vectors(stopwords):\n",
    "\n",
    "    # Word TF‑IDF\n",
    "    word_vec = TfidfVectorizer(\n",
    "        analyzer=\"word\", \n",
    "        ngram_range=WORD_NGRAMS,\n",
    "        min_df=MIN_DF, \n",
    "        max_df=MAX_DF, \n",
    "        max_features=MAXF_WORD,\n",
    "        lowercase=True, \n",
    "        strip_accents=\"unicode\", \n",
    "        sublinear_tf=True,\n",
    "        stop_words=stopwords\n",
    "    )\n",
    "\n",
    "    Xw_tr = word_vec.fit_transform(train_text)\n",
    "    Xw_te = word_vec.transform(test_text)\n",
    "\n",
    "    # Stack channels horizontally → [word]\n",
    "    X_tr = hstack([Xw_tr], format=\"csr\")\n",
    "    X_te = hstack([Xw_te], format=\"csr\")\n",
    "    return X_tr, X_te\n",
    "\n",
    "def select_top_k(X_tr, X_te):\n",
    "    \"\"\"χ²: keep top‑K label‑associated features (works with non‑negative TF‑IDF).\"\"\"\n",
    "    k_eff = min(K_SELECT, X_tr.shape[1])         # don’t request more than exist\n",
    "    selector = SelectKBest(chi2, k=k_eff)        # fit on TRAIN only\n",
    "    Xtr_sel = selector.fit_transform(X_tr, y)    # select best columns on train\n",
    "    Xte_sel = selector.transform(X_te)           # apply same mask to test\n",
    "    return Xtr_sel, Xte_sel\n",
    "\n",
    "def cv_auc_xgb(X):\n",
    "    \"\"\"5‑fold CV for XGBClassifier with early stopping; return mean/std AUC and best iter per fold.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    scores, best_iters = [], []\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        model = XGBClassifier(**XGB_PARAMS)\n",
    "        # train with early stopping on the validation fold (50 rounds patience)\n",
    "        model.fit(\n",
    "            X[tr_idx], y[tr_idx],\n",
    "            eval_set=[(X[va_idx], y[va_idx])],\n",
    "            verbose=False\n",
    "        )\n",
    "        p = model.predict_proba(X[va_idx])[:, 1]\n",
    "        scores.append(roc_auc_score(y[va_idx], p))\n",
    "        # model.best_iteration is set after early stopping; fallback to n_estimators if None\n",
    "        best_iters.append(int(getattr(model, \"best_iteration\", XGB_PARAMS[\"n_estimators\"])))\n",
    "    return {\n",
    "        \"auc_mean\": float(np.mean(scores)),\n",
    "        \"auc_std\":  float(np.std(scores)),\n",
    "        \"best_iters\": best_iters\n",
    "    }\n",
    "\n",
    "def evaluate_variant(name, stopwords):\n",
    "    \"\"\"Vectorize → χ² select → CV XGBoost → return results and selected features for final fit.\"\"\"\n",
    "    # 1) Vectorize\n",
    "    X_tr, X_te = build_vectors(stopwords)\n",
    "\n",
    "    # 2) χ² select\n",
    "    Xtr_sel, Xte_sel = select_top_k(X_tr, X_te)\n",
    "\n",
    "    # 3) CV with XGBoost\n",
    "    res = cv_auc_xgb(Xtr_sel)\n",
    "    print(f\"[{name}] XGB mean AUC={res['auc_mean']:.4f} (±{res['auc_std']:.4f}); \"\n",
    "          f\"best iters per fold: {res['best_iters']}\")\n",
    "    # keep artifacts for winner\n",
    "    res.update({\n",
    "        \"name\": name,\n",
    "        \"Xtr_sel\": Xtr_sel,\n",
    "        \"Xte_sel\": Xte_sel,\n",
    "        \"stopwords\": stopwords,\n",
    "    })\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- Evaluate all variants; pick the best by mean CV AUC ----\n",
    "results = [evaluate_variant(n, v[\"stopwords\"]) for n, v in variants.items()]\n",
    "winner = max(results, key=lambda r: r[\"auc_mean\"])\n",
    "print(f\"\\n>>> WINNER: {winner['name']} | XGB CV mean AUC = {winner['auc_mean']:.4f} (±{winner['auc_std']:.4f})\")\n",
    "\n",
    "# ---- Train final XGBoost on FULL train (use median of best_iterations from CV) ----\n",
    "best_n_estimators = int(np.median(winner[\"best_iters\"])) if len(winner[\"best_iters\"]) > 0 else XGB_PARAMS[\"n_estimators\"]\n",
    "final_params = {**XGB_PARAMS, \"n_estimators\": max(best_n_estimators, 50)}  # ensure >0 trees\n",
    "\n",
    "final = XGBClassifier(**final_params)\n",
    "final.fit(winner[\"Xtr_sel\"], y, verbose=False)             # fit once on all train features\n",
    "test_prob = final.predict_proba(winner[\"Xte_sel\"])[:, 1]   # probability of rule_violation\n",
    "\n",
    "# ---- Build submission: row_id + probability ----\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": test[\"row_id\"], \n",
    "    \"rule_violation\": test_prob\n",
    "})\n",
    "out_path = \"../outputs/sub_v2_tfidf_xgb.csv\"\n",
    "submission.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
