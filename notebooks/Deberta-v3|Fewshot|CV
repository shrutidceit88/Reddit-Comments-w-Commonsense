{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":2838907,"sourceType":"datasetVersion","datasetId":1737045},{"sourceId":2838978,"sourceType":"datasetVersion","datasetId":1737101}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shrutidceit/xplained-deberta-v3-fewshot-cv?scriptVersionId=258374158\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Check my earlier work which are my attempts before to the following notebook:\n- https://www.kaggle.com/code/shrutidceit/basics-xgboost-few-shot-learning?scriptVersionId=258173247\n- https://www.kaggle.com/code/shrutidceit/basic-cv-logistic-pipeline?scriptVersionId=257970214\n- https://www.kaggle.com/code/shrutidceit/tf-idf-logistic?scriptVersionId=256048235\n- https://www.kaggle.com/code/shrutidceit/tf-idf-logistic?scriptVersionId=256068857\n- https://www.kaggle.com/code/shrutidceit/complete-eda\n- https://www.kaggle.com/code/shrutidceit/tf-idf-logistic\n- https://www.kaggle.com/code/shrutidceit/detailed-eda-w-wordcloud\n\n<li>Feel free to share your Feedback - It's very important for me. \n<li>Please upvote to appreciate the attempt","metadata":{}},{"cell_type":"markdown","source":"### Importing necessary libraries\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os, gc, random\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nfrom torch.utils.data import Dataset\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:00:30.044144Z","iopub.execute_input":"2025-08-26T22:00:30.04445Z","iopub.status.idle":"2025-08-26T22:00:30.049318Z","shell.execute_reply.started":"2025-08-26T22:00:30.044426Z","shell.execute_reply":"2025-08-26T22:00:30.048563Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Let's import a set of building blocks from ***transformers library*** that will be used together for text classification tasks.\n\n<ul>\n    <li><strong>AutoTokenizer:</strong> It is a universal tokenizer loader from provided model (Deberta-v3 in this case) - Tokenizers turn raw text into input IDs (numbers) + attention masks so models can process them.</li>\n    <li><strong>AutoModelForSequenceClassification:</strong> Loads a pretrained Transformer model with a classification head attached. This head is randomly initialized and learns during training.</li>\n    <li><strong>DataCollatorWithPadding:</strong> Handles dynamic padding of batches during training. Since sentences are different lengths, this ensures all samples in a batch are padded to the same length on the fly</li>\n    <li><strong>Trainer:</strong> Hugging Face’s high-level training loop abstraction and handles: Forward/backward pass, Optimizer & scheduler, Evaluation loop, Saving/loading checkpoints</li>\n    <li><strong>TrainingArguments:</strong> Holds all the training hyperparameters and keepds code clean: learning_rate, num_train_epochs, batch_size, save_strategy, logging_steps</li>\n    <li><strong>set_seed:</strong> Set a random seed for reproducibility</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,                    \n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    Trainer,\n    TrainingArguments,\n    set_seed,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:00:35.93521Z","iopub.execute_input":"2025-08-26T22:00:35.935501Z","iopub.status.idle":"2025-08-26T22:01:05.022197Z","shell.execute_reply.started":"2025-08-26T22:00:35.935481Z","shell.execute_reply":"2025-08-26T22:01:05.021259Z"}},"outputs":[{"name":"stderr","text":"2025-08-26 22:00:41.393802: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756245641.596038      68 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756245641.656402      68 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Import Data files and Deberta-v3-base Model","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.023612Z","iopub.execute_input":"2025-08-26T22:01:05.024352Z","iopub.status.idle":"2025-08-26T22:01:05.059423Z","shell.execute_reply.started":"2025-08-26T22:01:05.024329Z","shell.execute_reply":"2025-08-26T22:01:05.058729Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv\n/kaggle/input/jigsaw-agile-community-rules/train.csv\n/kaggle/input/jigsaw-agile-community-rules/test.csv\n/kaggle/input/deberta-v3-large/deberta-v3-large/spm.model\n/kaggle/input/deberta-v3-large/deberta-v3-large/config.json\n/kaggle/input/deberta-v3-large/deberta-v3-large/README.md\n/kaggle/input/deberta-v3-large/deberta-v3-large/tokenizer_config.json\n/kaggle/input/deberta-v3-large/deberta-v3-large/pytorch_model.bin\n/kaggle/input/deberta-v3-base/deberta-v3-base/spm.model\n/kaggle/input/deberta-v3-base/deberta-v3-base/config.json\n/kaggle/input/deberta-v3-base/deberta-v3-base/README.md\n/kaggle/input/deberta-v3-base/deberta-v3-base/tokenizer_config.json\n/kaggle/input/deberta-v3-base/deberta-v3-base/pytorch_model.bin\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Importing *train* and *test* files as pandas df","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.060251Z","iopub.execute_input":"2025-08-26T22:01:05.060595Z","iopub.status.idle":"2025-08-26T22:01:05.13058Z","shell.execute_reply.started":"2025-08-26T22:01:05.060573Z","shell.execute_reply":"2025-08-26T22:01:05.129697Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Defining Model Name","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"/kaggle/input/deberta-v3-base/deberta-v3-base\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.132651Z","iopub.execute_input":"2025-08-26T22:01:05.132959Z","iopub.status.idle":"2025-08-26T22:01:05.136925Z","shell.execute_reply.started":"2025-08-26T22:01:05.132938Z","shell.execute_reply":"2025-08-26T22:01:05.136066Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Defining *training configuration hyperparameters*\n\n<ul>\n    <li><strong>SEED:</strong>Fixed random seed for reproducibility.</li>\n    <li><strong>N_FOLDS:</strong>Number of cross-validation folds.</li>\n    <li><strong>MAX_LEN:</strong>Maximum sequence length for tokenized inputs; Text longer than 384 tokens is truncated, shorter ones are padded.</li>\n    <li><strong>BATCH_TRAIN:</strong>Training batch size</li>\n    <li><strong>BATCH_EVAL:</strong>Batch size during evaluation</li>\n    <li><strong>EPOCHS:</strong>Number of passes through the whole dataset.</li>\n    <li><strong>LR:</strong>Learning rate</li>\n    <li><strong>WEIGHT_DECAY:</strong>L2 regularization(shrinks large weights) - Helps prevent overfitting.</li>\n    <li><strong>WARMUP_RATIO:</strong>Gradually increases LR for the first 10% of steps</li>\n    <li><strong>GRAD_ACC_STEPS:</strong>Gradient accumulation steps - If >1, simulates larger batch sizes by accumulating gradients over several steps before updating weights.</li>\n    <li><strong>FP16:</strong>If GPU is available, use mixed precision (FP16) training.</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"SEED      = 42\nN_FOLDS   = 5\nMAX_LEN   = 512        \nBATCH_TRAIN = 8       \nBATCH_EVAL  = 16\nEPOCHS      = 3        \nLR          = 2e-5\nWEIGHT_DECAY = 0.01\nWARMUP_RATIO = 0.1\nGRAD_ACC_STEPS = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.137885Z","iopub.execute_input":"2025-08-26T22:01:05.138269Z","iopub.status.idle":"2025-08-26T22:01:05.151016Z","shell.execute_reply.started":"2025-08-26T22:01:05.138247Z","shell.execute_reply":"2025-08-26T22:01:05.15014Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"FP16 = torch.cuda.is_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:55.102812Z","iopub.execute_input":"2025-08-26T22:01:55.10314Z","iopub.status.idle":"2025-08-26T22:01:55.107868Z","shell.execute_reply.started":"2025-08-26T22:01:55.103117Z","shell.execute_reply":"2025-08-26T22:01:55.106896Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"set_seed(SEED) # Sets all the relevant random seeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.151899Z","iopub.execute_input":"2025-08-26T22:01:05.152142Z","iopub.status.idle":"2025-08-26T22:01:05.168858Z","shell.execute_reply.started":"2025-08-26T22:01:05.152122Z","shell.execute_reply":"2025-08-26T22:01:05.168002Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### ***shorten()*** takes a string txt and trims it to atmost max_chars characters\n\n<ul>\n    <li>If the input is NaN/missing value, return an empty string.</li>\n    <li>Converts txt to string (in case it’s not) + .strip() removes leading and trailing spaces. + .replace(\"\\n\", \" \") replaces line breaks with spaces.</li>\n    <li>If the text is longer than max_chars → cut it to the first max_chars characters and append an ellipsis \"…\" ; Otherwise, return the text as is</li>\n</ul>\n\n***ellipsis:*** Adding \"…\" signals: “This text was cut, there’s more not shown.”","metadata":{}},{"cell_type":"code","source":"def _shorten(txt, max_chars=180):\n    if pd.isna(txt): \n        return \"\"\n    txt = str(txt).strip().replace(\"\\n\", \" \")\n    return (txt[:max_chars] + \"…\") if len(txt) > max_chars else txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.169827Z","iopub.execute_input":"2025-08-26T22:01:05.170138Z","iopub.status.idle":"2025-08-26T22:01:05.175028Z","shell.execute_reply.started":"2025-08-26T22:01:05.170106Z","shell.execute_reply":"2025-08-26T22:01:05.174091Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"#### ***build_text_a()*** takes one row from train/test dataframe and constructs and returns a formatted prompt-like string","metadata":{}},{"cell_type":"code","source":"def build_text_a(row, max_chars_each=160):\n    subreddit = _shorten(row.get(\"subreddit\", \"\"), max_chars_each)      \n    rules  = _shorten(row.get(\"rule\", \"\"), max_chars_each)\n    p1 = _shorten(row.get(\"positive_example_1\", \"\"), max_chars_each)\n    p2 = _shorten(row.get(\"positive_example_2\", \"\"), max_chars_each)\n    n1 = _shorten(row.get(\"negative_example_1\", \"\"), max_chars_each)\n    n2 = _shorten(row.get(\"negative_example_2\", \"\"), max_chars_each)\n    \n    text_a = f\"subreddit: {subreddit}\\nRule: {rules}\\nYes: {p1} | {p2}\\nNo: {n1} | {n2}\"\n    return text_a.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.175896Z","iopub.execute_input":"2025-08-26T22:01:05.17631Z","iopub.status.idle":"2025-08-26T22:01:05.188248Z","shell.execute_reply.started":"2025-08-26T22:01:05.17629Z","shell.execute_reply":"2025-08-26T22:01:05.187266Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Applying ***build_text_a()*** to every row in train/test DataFrames and store the returned string in a new column ***text_a***","metadata":{}},{"cell_type":"code","source":"train[\"text_a\"] = train.apply(build_text_a, axis=1)\ntest[\"text_a\"]  = test.apply(build_text_a, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.18926Z","iopub.execute_input":"2025-08-26T22:01:05.189888Z","iopub.status.idle":"2025-08-26T22:01:05.247181Z","shell.execute_reply.started":"2025-08-26T22:01:05.189856Z","shell.execute_reply":"2025-08-26T22:01:05.246417Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Create clean comment column (text_b) \n<li>It will be paired ---- text_a, text_b ----  for Deberta cross-encoder classifier.","metadata":{}},{"cell_type":"code","source":"train[\"text_b\"] = train[\"body\"].fillna(\"\").astype(str)\ntest[\"text_b\"]  = test[\"body\"].fillna(\"\").astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.249516Z","iopub.execute_input":"2025-08-26T22:01:05.249808Z","iopub.status.idle":"2025-08-26T22:01:05.256419Z","shell.execute_reply.started":"2025-08-26T22:01:05.24977Z","shell.execute_reply":"2025-08-26T22:01:05.255463Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Setting Labels to ***int***","metadata":{}},{"cell_type":"code","source":"train[\"label\"] = train[\"rule_violation\"].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.257405Z","iopub.execute_input":"2025-08-26T22:01:05.258174Z","iopub.status.idle":"2025-08-26T22:01:05.270988Z","shell.execute_reply.started":"2025-08-26T22:01:05.25815Z","shell.execute_reply":"2025-08-26T22:01:05.270141Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Load the right tokenizer for Deberta-v3","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:05.272009Z","iopub.execute_input":"2025-08-26T22:01:05.272339Z","iopub.status.idle":"2025-08-26T22:01:07.972348Z","shell.execute_reply.started":"2025-08-26T22:01:05.272316Z","shell.execute_reply":"2025-08-26T22:01:07.971611Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Creating class PairDataset \n\n<li>Creates a custom PyTorch Dataset.\n<li>This lets PyTorch’s DataLoader efficiently fetch, batch, and shuffle your training samples\n<li>Converts DataFrame rows into tokenized inputs the model understands.\n<li>Handles paired inputs (rule+examples vs. comment).","metadata":{}},{"cell_type":"code","source":"class PairDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len, with_labels=True):\n        self.df = df.reset_index(drop=True)  # input DataFrame with text_a, text_b, and maybe label.\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.with_labels = with_labels # True for training/validation and False for test (since test has no labels)\n\n    def __len__(self): # Tells PyTorch how many samples exist.\n        return len(self.df)\n\n    def __getitem__(self, idx): # Fetches one row of the DataFrame by index.\n        row = self.df.iloc[idx]\n        enc = self.tokenizer(\n            row[\"text_a\"] + \"\\n\" + row[\"text_b\"],\n            max_length=self.max_len,\n            truncation=True,               # truncates across both A and B (longest_first) - ensures neither A nor B overflows the max length.\n            padding=False,                 # let collator pad dynamically\n            return_tensors=\"pt\",\n        )\n        item = {k: v.squeeze(0) for k, v in enc.items()}\n        \n        if self.with_labels:\n            item[\"labels\"] = torch.tensor(row[\"label\"], dtype=torch.long)\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:07.973698Z","iopub.execute_input":"2025-08-26T22:01:07.973988Z","iopub.status.idle":"2025-08-26T22:01:07.980845Z","shell.execute_reply.started":"2025-08-26T22:01:07.973967Z","shell.execute_reply":"2025-08-26T22:01:07.980189Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### DataCollatorWithPadding\n\n<li>When batching samples for training, dynamically pad sequences in the batch using the tokenizer’s pad token, and return attention masks accordingly.\n<li>Every sequence gets padded to max_len (e.g. 512)","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:07.982777Z","iopub.execute_input":"2025-08-26T22:01:07.983022Z","iopub.status.idle":"2025-08-26T22:01:07.999236Z","shell.execute_reply.started":"2025-08-26T22:01:07.982994Z","shell.execute_reply":"2025-08-26T22:01:07.998515Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Sigmoid()","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:08.000025Z","iopub.execute_input":"2025-08-26T22:01:08.000282Z","iopub.status.idle":"2025-08-26T22:01:08.01057Z","shell.execute_reply.started":"2025-08-26T22:01:08.000261Z","shell.execute_reply":"2025-08-26T22:01:08.009938Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Cross-validation setup\n<li> Stratified fold ensures each fold has the same proportion of classes as the original dataset\n<li> Out-Of-Fold(OOF): This initializes a numpy array with zeros, length equal to your training set.\n<li> oof_preds → stores out-of-fold predictions to calculate a robust validation score.\n","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\noof_preds = np.zeros(len(train), dtype=float)\n\ntest_preds_folds = []\nfold_auc = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:10.116218Z","iopub.execute_input":"2025-08-26T22:01:10.116897Z","iopub.status.idle":"2025-08-26T22:01:10.121374Z","shell.execute_reply.started":"2025-08-26T22:01:10.116867Z","shell.execute_reply":"2025-08-26T22:01:10.120487Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"for fold, (trn_idx, val_idx) in enumerate(skf.split(train, train[\"label\"]), start=1):\n    print(f\"\\n===== Fold {fold}/{N_FOLDS} =====\")\n\n    df_trn = train.iloc[trn_idx].copy()\n    df_val = train.iloc[val_idx].copy()\n\n    ds_trn = PairDataset(df_trn, tokenizer, MAX_LEN, with_labels=True)\n    ds_val = PairDataset(df_val, tokenizer, MAX_LEN, with_labels=True)\n    ds_tst = PairDataset(test, tokenizer, MAX_LEN, with_labels=False)\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels=2,   # binary classification\n    )\n\n    args = TrainingArguments(\n    num_train_epochs=EPOCHS,\n    learning_rate=LR,\n    weight_decay=WEIGHT_DECAY,\n    warmup_ratio=WARMUP_RATIO,\n    per_device_train_batch_size=BATCH_TRAIN,\n    per_device_eval_batch_size=BATCH_EVAL,\n    gradient_accumulation_steps=GRAD_ACC_STEPS,\n    logging_steps=100,\n    save_strategy=\"no\",\n    report_to=\"none\",\n    fp16=FP16,\n    dataloader_num_workers=2,\n    seed=SEED + fold,\n)\n     \n    FP16 = torch.cuda.is_available()\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=ds_trn,\n        eval_dataset=None,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    # Train\n    trainer.train()\n\n    # Predict on validation\n    val_out = trainer.predict(ds_val)\n    \n    # val_out.predictions shape: (N, 2) logits\n    val_logits = val_out.predictions\n    val_probs = sigmoid(val_logits[:, 1] - val_logits[:, 0])  \n\n    # Store OOF\n    oof_preds[val_idx] = val_probs\n    val_auc = roc_auc_score(df_val[\"label\"].values, val_probs)\n    fold_auc.append(val_auc)\n    print(f\"Fold {fold} AUC: {val_auc:.5f}\")\n\n    # Predict on test for this fold\n    tst_out = trainer.predict(ds_tst)\n    tst_logits = tst_out.predictions\n    tst_probs  = sigmoid(tst_logits[:, 1] - tst_logits[:, 0])\n    test_preds_folds.append(tst_probs)\n\n    # Memory cleanup\n    del model, trainer, ds_trn, ds_val, ds_tst, val_out, tst_out\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:01:58.976565Z","iopub.execute_input":"2025-08-26T22:01:58.977357Z","iopub.status.idle":"2025-08-26T22:23:23.121994Z","shell.execute_reply.started":"2025-08-26T22:01:58.977331Z","shell.execute_reply":"2025-08-26T22:23:23.121109Z"}},"outputs":[{"name":"stdout","text":"\n===== Fold 1/5 =====\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-base/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [306/306 04:04, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.693600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.663200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.598300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 1 AUC: 0.78612\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n===== Fold 2/5 =====\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-base/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [306/306 04:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.685700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.642900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.529600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 2 AUC: 0.78374\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n===== Fold 3/5 =====\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-base/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [306/306 04:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.685100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.663100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.590900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 3 AUC: 0.75330\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n===== Fold 4/5 =====\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-base/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [306/306 04:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.690500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.668000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.586600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 4 AUC: 0.74821\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n===== Fold 5/5 =====\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/deberta-v3-base/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [306/306 04:04, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.684900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.652300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.558400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Fold 5 AUC: 0.75165\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"print(\"OOF AUC:\", roc_auc_score(train[\"label\"].values, oof_preds))\nprint(\"Fold AUCs:\", [f\"{x:.5f}\" for x in fold_auc])\nprint(\"Mean AUC:\", np.mean(fold_auc))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:23:23.12387Z","iopub.execute_input":"2025-08-26T22:23:23.124142Z","iopub.status.idle":"2025-08-26T22:23:23.133689Z","shell.execute_reply.started":"2025-08-26T22:23:23.124118Z","shell.execute_reply":"2025-08-26T22:23:23.132926Z"}},"outputs":[{"name":"stdout","text":"OOF AUC: 0.7582235275594837\nFold AUCs: ['0.78612', '0.78374', '0.75330', '0.74821', '0.75165']\nMean AUC: 0.7646023139005378\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"test_pred = np.mean(np.column_stack(test_preds_folds), axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:23:32.896815Z","iopub.execute_input":"2025-08-26T22:23:32.897141Z","iopub.status.idle":"2025-08-26T22:23:32.901702Z","shell.execute_reply.started":"2025-08-26T22:23:32.897117Z","shell.execute_reply":"2025-08-26T22:23:32.900855Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Final submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"row_id\": test[\"row_id\"],\n    \"rule_violation\": np.round(test_pred.astype(float), 2)\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T22:23:34.279541Z","iopub.execute_input":"2025-08-26T22:23:34.279885Z","iopub.status.idle":"2025-08-26T22:23:34.290488Z","shell.execute_reply.started":"2025-08-26T22:23:34.27986Z","shell.execute_reply":"2025-08-26T22:23:34.289828Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"   row_id  rule_violation\n0    2029               0\n1    2030               0\n2    2031               0\n3    2032               0\n4    2033               0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>rule_violation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2029</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2030</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2031</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2032</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2033</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28}]}