{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":10285957,"sourceType":"datasetVersion","datasetId":6365346},{"sourceId":12643252,"sourceType":"datasetVersion","datasetId":7916512},{"sourceId":12870970,"sourceType":"datasetVersion","datasetId":8141813},{"sourceId":12910937,"sourceType":"datasetVersion","datasetId":8033597}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T20:06:07.493929Z","iopub.execute_input":"2025-08-30T20:06:07.494171Z","iopub.status.idle":"2025-08-30T20:06:10.640109Z","shell.execute_reply.started":"2025-08-30T20:06:07.494145Z","shell.execute_reply":"2025-08-30T20:06:10.639250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Qwen2.5-0.5B cross-encoder, head-only training, 5-fold =====\n!pip -q install \"transformers==4.44.2\" \"accelerate==0.33.0\"\n\nimport os, gc, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    Trainer,\n    TrainingArguments,\n    set_seed,\n)\n\n# ---------------- Config ----------------\nDATA_DIR  = \"/kaggle/input/jigsaw-agile-community-rules\"\nTRAIN_CSV = f\"{DATA_DIR}/train.csv\"\n\nMODEL_ID  = \"Qwen/Qwen2.5-0.5B\"    # small & stable\nOUT_ROOT  = \"/kaggle/working/qwen25_ce_headonly\"\nos.makedirs(OUT_ROOT, exist_ok=True)\n\nSEED=42; N_FOLDS=5\nMAX_LEN=256\nEPOCHS=2\nTRAIN_BS=4\nGRAD_ACC=4\nLR=2e-4\nWARMUP=0.05\n\nset_seed(SEED)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ------------- Build paired inputs -------------\ndef _clip(x):\n    x = \"\" if pd.isna(x) else str(x).strip()\n    return x if 0 < len(x) < 300 else \"\"\n\ndef build_text_a(row):\n    sub  = str(row[\"subreddit\"])\n    rule = str(row[\"rule\"])\n    pos1 = _clip(row.get(\"positive_example_1\",\"\"))\n    neg1 = _clip(row.get(\"negative_example_1\",\"\"))\n    parts = [f\"r/{sub}\", f\"Rule: {rule}\"]\n    if pos1: parts.append(f\"Yes: {pos1}\")\n    if neg1: parts.append(f\"No: {neg1}\")\n    return \" | \".join(parts)\n\ndef prepare_df(df):\n    df = df.copy()\n    df[\"text_a\"] = df.apply(build_text_a, axis=1)\n    df[\"text_b\"] = df[\"body\"].astype(str)\n    return df\n\ndf = pd.read_csv(TRAIN_CSV)\ndf = prepare_df(df)\ny  = df[\"rule_violation\"].astype(int).values\n\n# ------------- Tokenizer (set PAD) -------------\ntok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True)\n# Use EOS as PAD if PAD missing\nif tok.pad_token is None:\n    if tok.eos_token is None:\n        tok.add_special_tokens({\"eos_token\":\"</s>\"})\n    tok.pad_token = tok.eos_token\ntok.padding_side = \"right\"  # OK for seq cls\n\ncollator = DataCollatorWithPadding(tokenizer=tok)\n\n# ------------- Dataset -------------\nclass PairDataset(Dataset):\n    def __init__(self, df, labels=None, max_len=256):\n        self.a = df[\"text_a\"].tolist()\n        self.b = df[\"text_b\"].tolist()\n        self.labels = labels\n        self.max_len = max_len\n    def __len__(self): return len(self.a)\n    def __getitem__(self, i):\n        item = tok(self.a[i], self.b[i], truncation=True, max_length=self.max_len)\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(int(self.labels[i]), dtype=torch.long)  # <-- important\n        return item\n\n# ------------- Model builder (freeze base, train head) -------------\ndef build_headonly_model():\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_ID,\n        num_labels=2,\n        trust_remote_code=True,\n        torch_dtype=torch.float32,\n    )\n    model.config.use_cache = False\n\n    # ensure PAD token id is set\n    model.config.pad_token_id = tok.pad_token_id\n    model.config.problem_type = \"single_label_classification\"\n\n    # keep embeddings in sync if special tokens were added\n    if model.get_input_embeddings().num_embeddings != len(tok):\n        model.resize_token_embeddings(len(tok))\n\n    # build fresh classifier head in SAME dtype/device as the base model\n    # base_dtype  = next(model.parameters()).dtype\n    # base_device = next(model.parameters()).device\n    hidden = model.config.hidden_size\n\n    model.score = torch.nn.Linear(hidden, 2) # , bias=True, device=base_device, dtype=base_dtype\n    torch.nn.init.xavier_uniform_(model.score.weight)\n    torch.nn.init.zeros_(model.score.bias)\n\n    # freeze everything except the classifier head\n    for n, p in model.named_parameters():\n        p.requires_grad = (\"score\" in n)\n\n    return model.to(device)\n\n\n\n# ------------- CV Train -------------\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\noof = np.zeros(len(df), dtype=float)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df, y), 1):\n    print(f\"\\n===== Fold {fold}/{N_FOLDS} =====\")\n    dtr = PairDataset(df.iloc[trn_idx], y[trn_idx], MAX_LEN)\n    dvl = PairDataset(df.iloc[val_idx], y[val_idx], MAX_LEN)\n\n    model = build_headonly_model()\n\n    \n    \n    args = TrainingArguments(\n        output_dir=f\"{OUT_ROOT}/fold{fold}\",\n        num_train_epochs=EPOCHS,\n        per_device_train_batch_size=TRAIN_BS,\n        gradient_accumulation_steps=GRAD_ACC,\n        learning_rate=LR,\n        warmup_ratio=WARMUP,\n        weight_decay=0.01,\n        logging_steps=50,\n        save_strategy=\"no\",\n        report_to=\"none\",\n        remove_unused_columns=False,\n        fp16=(device==\"cuda\"),\n        dataloader_pin_memory=False,\n        seed=SEED,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=dtr,\n        tokenizer=tok,\n        data_collator=collator,\n    )\n    trainer.train()\n\n    # ðŸ”¹ Save model + tokenizer for this fold\n    trainer.model.save_pretrained(f\"{OUT_ROOT}/fold{fold}\")\n    tok.save_pretrained(f\"{OUT_ROOT}/fold{fold}\")\n    print(f\"Saved fold {fold} to {OUT_ROOT}/fold{fold}\")\n\n    # ---------- OOF AUC ----------\n    model.eval()\n    dl = DataLoader(dvl, batch_size=128, shuffle=False, collate_fn=collator)\n    \n    preds = []\n    with torch.no_grad():\n        for batch in dl:\n            for k in batch:\n                batch[k] = batch[k].to(device)\n            logits_fp32 = model(**batch).logits.float()\n            logits_fp32 = torch.nan_to_num(logits_fp32, nan=0.0, posinf=1e4, neginf=-1e4)\n            prob = torch.softmax(logits_fp32, dim=1)[:, 1]\n            prob = torch.nan_to_num(prob, nan=0.5)\n            preds.append(prob.detach().cpu().numpy())\n\n    prob1 = np.concatenate(preds)\n    prob1 = np.clip(prob1, 0.0, 1.0)\n    if np.isnan(prob1).any():\n        prob1 = np.nan_to_num(prob1, nan=0.5)\n\n    oof[val_idx] = prob1\n    print(f\"Fold {fold} AUC: {roc_auc_score(y[val_idx], prob1):.4f}\")\n\n    del trainer, model, dtr, dvl, dl, preds, prob1\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\nprint(f\"\\nOOF AUC: {roc_auc_score(y, oof):.4f}\")\npd.DataFrame({\"oof\": oof, \"y\": y}).to_csv(f\"{OUT_ROOT}/oof.csv\", index=False)\nprint(\"Saved folds under:\", OUT_ROOT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T20:06:10.641077Z","iopub.execute_input":"2025-08-30T20:06:10.641482Z","iopub.status.idle":"2025-08-30T20:31:03.238897Z","shell.execute_reply.started":"2025-08-30T20:06:10.641460Z","shell.execute_reply":"2025-08-30T20:31:03.238082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, subprocess, textwrap\nsubprocess.run([\"bash\",\"-lc\",\"ls -R /kaggle/working/qwen25_ce_headonly | sed -n '1,200p'\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T20:31:03.240673Z","iopub.execute_input":"2025-08-30T20:31:03.240910Z","iopub.status.idle":"2025-08-30T20:31:03.325596Z","shell.execute_reply.started":"2025-08-30T20:31:03.240886Z","shell.execute_reply":"2025-08-30T20:31:03.324858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}